{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_optimal_stopping.ipynb\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import cholesky\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingPolicy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    3层神经网络,包含批量归一化和Xavier初始化\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=40):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        # Xavier初始化\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathGenerator:\n",
    "    \"\"\"路径生成的抽象基类\"\"\"\n",
    "    def __init__(self, d, steps, T):\n",
    "        self.d = d          # 维度\n",
    "        self.steps = steps  # 时间步数\n",
    "        self.T = T         # 总时间\n",
    "\n",
    "    def generate(self, n_paths):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesGenerator(PathGenerator):\n",
    "    \"\"\"\n",
    "    多维Black-Scholes路径生成器\n",
    "    \"\"\"\n",
    "    def __init__(self, d, steps, T, r, sigma, rho, div):\n",
    "        super().__init__(d, steps, T)\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.div = div\n",
    "        self.dt = T / steps\n",
    "        self.drift = (self.r - self.div - 0.5 * self.sigma**2) * self.dt  # 新增此行\n",
    "        self._build_cov_matrix()\n",
    "        \n",
    "    def _build_cov_matrix(self):\n",
    "        \"\"\"构建相关系数矩阵\"\"\"\n",
    "        # 构造单位时间协方差矩阵（不乘 dt）\n",
    "        cov = np.eye(self.d) * self.sigma**2\n",
    "        cov += np.ones((self.d, self.d)) * (self.rho * self.sigma**2)\n",
    "        np.fill_diagonal(cov, self.sigma**2)  # 防止对角重复加\n",
    "\n",
    "        # 然后 Cholesky 分解\n",
    "        self.L = torch.tensor(cholesky(cov, lower=True), device=device, dtype=torch.float32)\n",
    "        \n",
    "    def generate(self, n_paths):\n",
    "        \"\"\"生成路径 如有GPU可加速 \"\"\"\n",
    "        paths = torch.zeros(n_paths, self.steps+1, self.d, device=device, dtype=torch.float32)\n",
    "        paths[:, 0] = 100.0  # 初始价格\n",
    "        \n",
    "        noise = torch.randn(n_paths, self.steps, self.d, device=device)\n",
    "        \n",
    "        sqrt_dt = torch.tensor(np.sqrt(self.dt), device=device, dtype=torch.float32)\n",
    "        with tqdm(total=self.steps, desc=\"生成Black-Scholes路径\", leave=False) as pbar:\n",
    "            for t in range(1, self.steps+1):\n",
    "                increments = self.drift + (self.L @ noise[:, t-1].T).T* sqrt_dt\n",
    "                paths[:, t] = paths[:, t-1] * torch.exp(increments)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepOptimalStoppingTrainer:\n",
    "    \"\"\"\n",
    "    实现论文第2节的递归训练算法\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, hidden_dim=40):\n",
    "        self.generator = generator\n",
    "        self.policies = self._init_policies(hidden_dim)\n",
    "        self.optimizers = [torch.optim.Adam(p.parameters(), lr=0.001) \n",
    "                          for p in self.policies]\n",
    "        \n",
    "    def _init_policies(self, hidden_dim):\n",
    "        input_dim = self.generator.d + 1  # 状态+收益\n",
    "        # 初始化 steps+1 个策略网络（对应时间步 0~steps）\n",
    "        return [StoppingPolicy(input_dim, hidden_dim).to(device)\n",
    "                for _ in range(self.generator.steps + 1)]  # 修改为 steps+1\n",
    "    \n",
    "    def train(self, n_paths=8192, epochs=3000, batch_size=2048):\n",
    "        \"\"\"执行反向递归训练\"\"\"\n",
    "        paths = self.generator.generate(n_paths)\n",
    "        # 从到期日向前递归训练\n",
    "        for step in tqdm(reversed(range(self.generator.steps)), desc=\"Training steps\"):\n",
    "            # 动态计算当前时间步的收益\n",
    "            current_payoff = self._compute_payoffs(paths, start_step=step)[:, step]\n",
    "            X, y = self._prepare_training_data(paths, current_payoff, step)\n",
    "            self._train_single_step(step, X, y, epochs, batch_size)\n",
    "    \n",
    "    def _compute_payoffs(self, paths):\n",
    "        \"\"\"计算各时间步的即时收益（需子类实现）\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _prepare_training_data(self, paths, current_payoff, step):\n",
    "        \"\"\"准备训练数据（含嵌套蒙特卡洛）\"\"\"\n",
    "        current_state = paths[:, step]\n",
    "        X = torch.cat([current_state, current_payoff.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # 嵌套蒙特卡洛计算继续价值（传递起始时间步）\n",
    "        continuation = self._nested_mc(paths, step)\n",
    "        y = (current_payoff >= continuation).float()\n",
    "        return X, y\n",
    "\n",
    "    def _nested_mc(self, paths, step, J=16384):\n",
    "        \"\"\"嵌套蒙特卡洛模拟，传递起始时间步\"\"\"\n",
    "        n_paths = paths.size(0)\n",
    "        continuation = torch.zeros(n_paths, device=device)\n",
    "        \n",
    "        with tqdm(total=n_paths, desc=f\"嵌套MC(t={step})\", leave=False) as main_pbar:\n",
    "            for i in range(n_paths):\n",
    "                # 生成J条继续路径（从当前step开始）\n",
    "                new_paths = self._generate_continuations(paths[i, step], step, J)\n",
    "                \n",
    "                # 应用后续策略，传递起始时间步\n",
    "                exercise_times = self._apply_policies(new_paths, start_step=step)\n",
    "                \n",
    "                # 计算继续路径的收益，起始时间步为step\n",
    "                payoffs = self._compute_continuation_payoffs(new_paths, exercise_times, step)\n",
    "                \n",
    "                continuation[i] = payoffs.mean()\n",
    "                main_pbar.update(1)\n",
    "        \n",
    "        return continuation\n",
    "    \n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"从当前状态生成继续路径（需子类实现）\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _apply_policies(self, paths, start_step):\n",
    "        \"\"\"应用策略网络（修正时间步映射），并打印调试信息\"\"\"\n",
    "        batch_size = paths.size(0)\n",
    "        # 初始设定所有路径的行权时刻为最后一步\n",
    "        exercise_times = torch.full((batch_size,), self.generator.steps, device=device)\n",
    "        \n",
    "        # 继续路径的本地时间步数（注意：paths 中第0步为当前状态）\n",
    "        total_local_steps = paths.size(1)\n",
    "        #print(f\"[DEBUG] start_step: {start_step}, total_local_steps: {total_local_steps}\")\n",
    "        \n",
    "        # 遍历从本地时间步1开始（因为第0步为当前状态）\n",
    "        for local_t in range(1, total_local_steps):\n",
    "            global_t = start_step + local_t  # 将局部时间步映射为全局时间步\n",
    "            if global_t > self.generator.steps:\n",
    "                break  \n",
    "            states = paths[:, local_t]  # shape: [batch_size, d]\n",
    "            \n",
    "            # 截取从起始时间步到当前局部时间步的部分路径，\n",
    "            # 用于计算折现 payoff，注意这里传入的 start_step 保持全局时间参考\n",
    "            partial_paths = paths[:, :local_t+1]  # shape: [batch_size, local_t+1, d]\n",
    "            computed_payoffs = self._compute_payoffs(partial_paths, start_step=start_step)\n",
    "            # computed_payoffs 的 shape 为 [batch_size, local_t+1]，取最后一列作为当前时刻的 payoff\n",
    "            payoffs = computed_payoffs[:, local_t]\n",
    "            \n",
    "            # debug输出：检查当前局部时间步的平均 payoff\n",
    "            avg_payoff = payoffs.mean().item()\n",
    "            \n",
    "            # 拼接状态与 payoff 作为策略网络的输入（假设输入维度为 d+1）\n",
    "            inputs = torch.cat([states, payoffs.unsqueeze(1)], dim=1)\n",
    "            stop_probs = self.policies[global_t](inputs).squeeze() \n",
    "            avg_stop_prob = stop_probs.mean().item()\n",
    "\n",
    "            stop_decisions = (stop_probs >= 0.5).float()\n",
    "            count_stop = (stop_decisions == 1).sum().item()\n",
    "            \n",
    "            #print(f\"[DEBUG] global_t: {global_t}, local_t: {local_t}, avg_payoff: {avg_payoff:.4f}, avg_stop_prob: {avg_stop_prob:.4f}, stops: {count_stop}/{batch_size}\")\n",
    "            \n",
    "            mask = (exercise_times == self.generator.steps) & (stop_decisions == 1)\n",
    "            exercise_times[mask] = global_t\n",
    "        \n",
    "        #print(f\"[DEBUG] Final exercise_times: {exercise_times}\")\n",
    "        return exercise_times\n",
    "    '''\n",
    "    def _apply_policies(self, paths, start_step):\n",
    "        \"\"\"应用训练好的策略网络 论文公式5 \"\"\"\n",
    "        exercise_times = torch.full((paths.size(0),), self.generator.steps, device=device)\n",
    "        for t in range(start_step + 1, paths.size(1)):\n",
    "            states = paths[:, t]\n",
    "            #print(\"是这里吗\")\n",
    "            # 动态计算当前时间步的收益，起始时间步为start_step\n",
    "            payoffs = self._compute_payoffs(\n",
    "                paths[:, :t+1],  # 截取到当前时间步的路径\n",
    "                start_step=start_step\n",
    "            )[:, t - start_step]  # 局部时间步索引\n",
    "            \n",
    "            inputs = torch.cat([states, payoffs.unsqueeze(1)], dim=1)\n",
    "            stop_probs = self.policies[t](inputs).squeeze()\n",
    "            stop_decisions = (stop_probs >= 0.5).float()\n",
    "            \n",
    "            mask = (exercise_times == self.generator.steps) & (stop_decisions == 1)\n",
    "            exercise_times[mask] = t\n",
    "        \n",
    "        return exercise_times\n",
    "    '''\n",
    "        \n",
    "    def _train_single_step(self, step, X, y, epochs, batch_size):\n",
    "        \"\"\"训练单个时间步的策略网络\"\"\"\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=f\"Training step {step}\", leave=False):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizers[step].zero_grad()\n",
    "                pred = self.policies[step](batch_X).squeeze()\n",
    "                loss = torch.nn.BCELoss()(pred, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizers[step].step()\n",
    "\n",
    "    def compute_lower_bound(self, n_paths=4096000, conf_level=0.95):\n",
    "        \"\"\"计算下界及置信区间 论文3.1节 \"\"\"\n",
    "        # 生成独立路径集\n",
    "        test_paths = self.generator.generate(n_paths)\n",
    "        \n",
    "        # 应用训练好的策略确定停止时间\n",
    "        exercise_times = self._apply_policies(test_paths, 0)\n",
    "        \n",
    "        # 收集收益\n",
    "        payoffs = torch.zeros(n_paths, device=device)\n",
    "        for i in range(n_paths):\n",
    "            t = int(exercise_times[i].item())\n",
    "            payoffs[i] = self._compute_payoffs(test_paths[i:i+1])[0, t]\n",
    "        \n",
    "        # 计算统计量\n",
    "        mean = payoffs.mean().item()\n",
    "        std = payoffs.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1 + conf_level)/2)\n",
    "        ci_half = z * std / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean,\n",
    "            'lower': mean - ci_half,\n",
    "            'upper': mean + ci_half,\n",
    "            'std': std\n",
    "        }\n",
    "    \n",
    "    def compute_upper_bound(self, n_paths=1024, J=16384, conf_level=0.95):\n",
    "        \"\"\"计算上界及置信区间\"\"\"\n",
    "        # 生成独立主路径\n",
    "        primary_paths = self.generator.generate(n_paths)\n",
    "        payoffs = self._compute_payoffs(primary_paths, start_step=0)  # shape: [n_paths, steps+1]\n",
    "        steps = self.generator.steps  \n",
    "        \n",
    "        M = torch.zeros(n_paths, steps + 1, device=device)\n",
    "        # 修改：用 t=0 时的立即 payoff 作为初始条件期望\n",
    "        E_H_prev = payoffs[:, 0].clone()  # 形状 (n_paths,)\n",
    "        \n",
    "        for t in tqdm(range(1, steps+1), desc=\"Building Martingale\"):\n",
    "            # 当前用主路径在 t-1 时刻的状态生成继续路径\n",
    "            steps_remaining = steps - (t - 1)  \n",
    "            time_points = steps_remaining + 1  \n",
    "            \n",
    "            # 初始化继续路径\n",
    "            cont_paths = torch.zeros(n_paths, J, time_points, self.generator.d, device=device)\n",
    "            cont_values = torch.zeros(n_paths, J, device=device)\n",
    "            \n",
    "            for i in range(n_paths):\n",
    "                current_state = primary_paths[i, t-1]\n",
    "                cont_paths[i] = self._generate_continuations(current_state, t-1, J)\n",
    "                ex_times = self._apply_policies(cont_paths[i], start_step=t-1)\n",
    "                cont_values[i] = self._compute_continuation_payoffs(\n",
    "                    cont_paths[i], \n",
    "                    ex_times, \n",
    "                    current_step=t-1\n",
    "                )\n",
    "            \n",
    "            # 计算条件期望：E_H_t = E[g(τ, continuation path) | X_{t-1}]\n",
    "            E_H_t = cont_values.mean(dim=1)  # shape: (n_paths,)\n",
    "            \n",
    "            # 修改：使用立即 payoff 在 t-1 与条件期望比较，取正部作为马氏鞅增量\n",
    "            delta_M = torch.clamp(payoffs[:, t-1] - E_H_prev, min=0)\n",
    "            M[:, t] = M[:, t-1] + delta_M\n",
    "            \n",
    "            # 更新条件期望缓存：用继续模拟得到的均值\n",
    "            E_H_prev = E_H_t.clone()\n",
    "            \n",
    "            #debug\n",
    "            avg_cont = E_H_prev.mean().item()\n",
    "            avg_payoff = payoffs[:, t-1].mean().item()\n",
    "            print(f\"时间步 {t}: 平均 immediate payoff = {avg_payoff:.4f}, \"\n",
    "                f\"平均 continuation value = {avg_cont:.4f}, \"\n",
    "                f\"平均 ΔM = {delta_M.mean().item():.4f}\")\n",
    "        \n",
    "        # 计算调整后的收益：对于每条主路径，取 max_{0<=t<=steps} { payoffs[t] - M[t] }\n",
    "        adjusted_payoffs = torch.zeros(n_paths, device=device)\n",
    "        for i in range(n_paths):\n",
    "            adjusted_payoffs[i] = torch.max(payoffs[i] - M[i, :payoffs.size(1)])\n",
    "        \n",
    "        mean_est = adjusted_payoffs.mean().item()\n",
    "        std_est = adjusted_payoffs.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1+conf_level)/2)\n",
    "        ci_half = z * std_est / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean_est,\n",
    "            'lower': mean_est - ci_half,\n",
    "            'upper': mean_est + ci_half,\n",
    "            'std': std_est\n",
    "        }\n",
    "\n",
    "    '''\n",
    "    def compute_upper_bound(self, n_paths=1024, J=16384, conf_level=0.95):\n",
    "        \"\"\"计算上界及置信区间（严格遵循对偶公式）\"\"\"\n",
    "        # 生成独立主路径\n",
    "        primary_paths = self.generator.generate(n_paths)\n",
    "        payoffs = self._compute_payoffs(primary_paths, start_step=0)\n",
    "        \n",
    "        # 初始化鞅过程 M 和条件期望缓存\n",
    "        M = torch.zeros(n_paths, self.generator.steps + 1, device=device)\n",
    "        E_H_prev = torch.zeros(n_paths, device=device)\n",
    "        \n",
    "        for t in tqdm(range(self.generator.steps + 1), desc=\"Building Martingale\"):\n",
    "            if t == 0:\n",
    "                H_0 = self.compute_lower_bound(n_paths=n_paths)['estimate']\n",
    "                M[:, 0] = 0\n",
    "                E_H_prev[:] = H_0\n",
    "                continue\n",
    "            \n",
    "            # 关键修正：计算正确的时间步数\n",
    "            steps_remaining = self.generator.steps - (t-1)  # 剩余时间间隔数\n",
    "            time_points = steps_remaining + 1  # 时间点数 = 间隔数 + 1\n",
    "            \n",
    "            # 初始化继续路径张量（修正第三维为time_points）\n",
    "            cont_paths = torch.zeros(\n",
    "                n_paths, \n",
    "                J, \n",
    "                time_points,  # 正确的时间点数\n",
    "                self.generator.d, \n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # 生成继续路径并计算条件期望\n",
    "            cont_values = torch.zeros(n_paths, J, device=device)\n",
    "            for i in range(n_paths):\n",
    "                current_state = primary_paths[i, t-1]\n",
    "                cont_paths[i] = self._generate_continuations(current_state, t-1, J)\n",
    "                ex_times = self._apply_policies(cont_paths[i], start_step=t-1)\n",
    "                cont_values[i] = self._compute_continuation_payoffs(\n",
    "                    cont_paths[i], \n",
    "                    ex_times, \n",
    "                    current_step=t-1\n",
    "                )\n",
    "            \n",
    "            # 计算条件期望 E[H_t | F_{t-1}] = mean(cont_values)\n",
    "            E_H_t = cont_values.mean(dim=1)  # 形状 (n_paths,)\n",
    "            \n",
    "            # 计算当前H_{t-1}（即主路径在t-1时刻的收益或继续价值）\n",
    "            H_t_minus_1 = torch.maximum(payoffs[:, t-1], E_H_prev)\n",
    "            \n",
    "            avg_cont = E_H_prev.mean().item()\n",
    "            avg_payoff = H_t_minus_1.mean().item()\n",
    "            print(f\"时间步 {t}: 平均主路径 payoff = {avg_payoff:.4f}, 平均 continuation value = {avg_cont:.4f}\")\n",
    "\n",
    "            # 更新鞅增量：M_t - M_{t-1} = H_t - E[H_t | F_{t-1}]\n",
    "            delta_M = H_t_minus_1 - E_H_prev\n",
    "            M[:, t] = M[:, t-1] + delta_M\n",
    "            \n",
    "            # 更新前一时间步的条件期望\n",
    "            E_H_prev = E_H_t\n",
    "        \n",
    "        # 计算调整后的收益：max(g(n,X_n) - M_n)\n",
    "        adjusted_payoffs = torch.zeros(n_paths, device=device)\n",
    "        for i in range(n_paths):\n",
    "            path_payoffs = payoffs[i]  # 主路径各时间步的收益\n",
    "            adjusted_payoffs[i] = torch.max(path_payoffs - M[i, :len(path_payoffs)])\n",
    "        \n",
    "        # 计算统计量\n",
    "        mean = adjusted_payoffs.mean().item()\n",
    "        std = adjusted_payoffs.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1 + conf_level)/2)\n",
    "        ci_half = z * std / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean,\n",
    "            'lower': mean - ci_half,\n",
    "            'upper': mean + ci_half,\n",
    "            'std': std\n",
    "        }\n",
    "    '''\n",
    "    '''\n",
    "    def compute_upper_bound(self, n_paths=1024, J=16384, conf_level=0.95):\n",
    "        \"\"\"计算上界及置信区间 论文3.2节双重方法\"\"\"\n",
    "        # 生成主路径，确保生成的路径形状为 [n_paths, steps+1, d]\n",
    "        primary_paths = self.generator.generate(n_paths)\n",
    "        payoffs = self._compute_payoffs(primary_paths)  # shape: [n_paths, steps+1]\n",
    "        \n",
    "        # 初始化Martingale, M[:,0] = 0，M的形状 [n_paths, steps+1]\n",
    "        M = torch.zeros(n_paths, self.generator.steps + 1, device=device)\n",
    "        \n",
    "        # 对于每个时间步 t 从 0 到 steps-1 进行循环\n",
    "        for t in range(self.generator.steps):\n",
    "            steps_remaining = self.generator.steps - t  # 剩余的时间步数\n",
    "            # 构造 continuation paths：形状 [n_paths, J, steps_remaining+1, d]\n",
    "            cont_paths = torch.zeros(\n",
    "                n_paths,\n",
    "                J,\n",
    "                steps_remaining + 1,  # 注意：这里+1使得包含起始时刻\n",
    "                self.generator.d,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # 为每条主路径生成对应的 continuation paths\n",
    "            for i in range(n_paths):\n",
    "                # _generate_continuations 接受当前状态 primary_paths[i,t] 和当前时间 t，\n",
    "                # 返回形状 [J, steps_remaining+1, d] 的 continuation paths\n",
    "                cont_paths[i] = self._generate_continuations(primary_paths[i, t], t, J)\n",
    "            \n",
    "            # 计算每条主路径在 t 时刻的继续价值 cont_value\n",
    "            cont_values = torch.zeros(n_paths, device=device)\n",
    "            for i in range(n_paths):\n",
    "                # 对于每个主路径，使用生成的 continuation paths 和当前时间 t，\n",
    "                # 计算停止策略下的停止时刻\n",
    "                ex_times = self._apply_policies(cont_paths[i], t)\n",
    "                # 计算 continuation payoff（需要确保 _compute_continuation_payoffs 正确使用 current_step=t 进行折现）\n",
    "                # 返回的是一个 [J] 维的张量，再取均值\n",
    "                cont_payoffs = self._compute_continuation_payoffs(cont_paths[i], ex_times, current_step=t)\n",
    "                cont_values[i] = cont_payoffs.mean()\n",
    "            \n",
    "            # Debug\n",
    "            current_payoff = payoffs[:, t]  # shape: [n_paths]\n",
    "            avg_cont = cont_values.mean().item()\n",
    "            avg_payoff = current_payoff.mean().item()\n",
    "            print(f\"时间步 {t}: 平均主路径 payoff = {avg_payoff:.4f}, 平均 continuation value = {avg_cont:.4f}\")\n",
    "            \n",
    "            delta_M = (current_payoff >= cont_values).float() * (current_payoff - cont_values)\n",
    "            M[:, t + 1] = M[:, t] + delta_M\n",
    "        \n",
    "        # 调整后的收益：payoff - martingale，取路径上的最大值\n",
    "        adjusted = payoffs - M\n",
    "        max_values, _ = torch.max(adjusted, dim=1)\n",
    "        \n",
    "        # 计算统计量\n",
    "        mean = max_values.mean().item()\n",
    "        std = max_values.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1 + conf_level) / 2)\n",
    "        ci_half = z * std / np.sqrt(n_paths)\n",
    "        \n",
    "        print(f\"上界估计: {mean:.4f}, 95% CI: [{mean - ci_half:.4f}, {mean + ci_half:.4f}]\")\n",
    "        return {\n",
    "            'estimate': mean,\n",
    "            'lower': mean - ci_half,\n",
    "            'upper': mean + ci_half,\n",
    "            'std': std\n",
    "        }\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BermudanMaxCallTrainer(DeepOptimalStoppingTrainer):\n",
    "    \"\"\"实现论文第4.1节的Bermudan Max-Call实验\"\"\"\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        # paths shape: (n_paths, L, d)，其中 L = 实际时间点数\n",
    "        max_values, _ = torch.max(paths, dim=2)  # (n_paths, L)\n",
    "        L = paths.size(1)  # 实际时间点数\n",
    "        # 用离散时间步构造时间向量：start_time, start_time+dt, …, start_time+(L-1)*dt\n",
    "        time_points = start_step * self.generator.dt + torch.arange(L, device=device, dtype=torch.float32) * self.generator.dt\n",
    "        discounts = torch.exp(-self.generator.r * time_points)\n",
    "        return (max_values - 100).clamp(min=0) * discounts  # K=100\n",
    "\n",
    "    '''\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        max_values, _ = torch.max(paths, dim=2)  # (n_paths, steps+1)\n",
    "        steps_remaining = paths.size(1) - 1  # 路径的局部时间步数\n",
    "        \n",
    "        start_time = start_step * self.generator.dt\n",
    "        time_points = torch.linspace(start_time, self.generator.T, steps_remaining + 1, device=device)\n",
    "        discounts = torch.exp(-self.generator.r * time_points)\n",
    "        return (max_values - 100).clamp(min=0) * discounts  # K=100\n",
    "'''\n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"生成继续路径，覆盖从step到steps的所有时间点（含steps）\"\"\"\n",
    "        steps_remaining = self.generator.steps - step  # 剩余时间间隔数\n",
    "        # 需要生成steps_remaining+1个时间点（包含step到steps）\n",
    "        new_paths = torch.zeros(J, steps_remaining + 1, self.generator.d, device=device)\n",
    "        new_paths[:, 0] = current_state\n",
    "        \n",
    "        for t in range(1, steps_remaining + 1):  # 生成steps_remaining个增量步骤\n",
    "            Z = torch.randn(J, self.generator.d, device=device)\n",
    "            increments = self.generator.drift + (self.generator.L @ Z.T).T * np.sqrt(self.generator.dt)\n",
    "            new_paths[:, t] = new_paths[:, t-1] * torch.exp(increments)\n",
    "        return new_paths\n",
    "\n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        # new_paths shape: (batch_size, L, d)，其中 L = steps_remaining + 1\n",
    "        batch_size, L, _ = new_paths.shape\n",
    "        time_points = current_step * self.generator.dt + torch.arange(L, device=device, dtype=torch.float32) * self.generator.dt\n",
    "        # 排除当前时刻（index 0）\n",
    "        discounts = torch.exp(-self.generator.r * time_points)[1:]\n",
    "        \n",
    "        max_values, _ = torch.max(new_paths, dim=2)  # (batch_size, L)\n",
    "        payoffs = (max_values[:, 1:] - 100).clamp(min=0) * discounts  # 从 current_step+1 开始\n",
    "        \n",
    "        selected_payoffs = torch.zeros(batch_size, device=device)\n",
    "        for i in range(batch_size):\n",
    "            absolute_t = int(exercise_times[i].item())\n",
    "            if absolute_t == self.generator.steps:  # 到期时执行\n",
    "                selected_payoffs[i] = payoffs[i, -1]\n",
    "            else:\n",
    "                relative_t = absolute_t - (current_step + 1)\n",
    "                if relative_t < 0 or relative_t >= (L - 1):\n",
    "                    raise ValueError(f\"Invalid exercise time {absolute_t} at step {current_step}\")\n",
    "                selected_payoffs[i] = payoffs[i, relative_t]\n",
    "        \n",
    "        return selected_payoffs\n",
    "\n",
    "    '''\n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        batch_size, steps_remaining_plus1, _ = new_paths.shape\n",
    "        steps_remaining = steps_remaining_plus1 - 1  # 时间间隔数\n",
    "        \n",
    "        # 时间点从current_step到current_step + steps_remaining（含两端）\n",
    "        time_points = torch.linspace(\n",
    "            current_step * self.generator.dt,\n",
    "            self.generator.T,\n",
    "            steps_remaining_plus1,\n",
    "            device=device\n",
    "        )\n",
    "        discounts = torch.exp(-self.generator.r * time_points)[1:]  # 排除current_step\n",
    "        \n",
    "        max_values, _ = torch.max(new_paths, dim=2)  # (batch_size, steps_remaining+1)\n",
    "        payoffs = (max_values[:, 1:] - 100).clamp(min=0) * discounts  # 从current_step+1开始\n",
    "        \n",
    "        selected_payoffs = torch.zeros(batch_size, device=device)\n",
    "        for i in range(batch_size):\n",
    "            absolute_t = int(exercise_times[i].item())\n",
    "            if absolute_t == self.generator.steps:  # 到期日执行\n",
    "                selected_payoffs[i] = payoffs[i, -1]\n",
    "            else:\n",
    "                # 转换为继续路径的局部索引（从current_step+1开始）\n",
    "                relative_t = absolute_t - (current_step + 1)\n",
    "                if relative_t < 0 or relative_t >= steps_remaining:\n",
    "                    raise ValueError(f\"Invalid exercise time {absolute_t} at step {current_step}\")\n",
    "                selected_payoffs[i] = payoffs[i, relative_t]\n",
    "        \n",
    "        return selected_payoffs\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBRCGenerator(BlackScholesGenerator):\n",
    "    \"\"\"带股息支付和障碍监测的路径生成器\"\"\"\n",
    "    def __init__(self, d, steps, T, r, sigma, rho, div_dates, div_rates, barriers):\n",
    "        super().__init__(d, steps, T, r, sigma, rho, div=0)\n",
    "        self.div_dates = div_dates  # 股息支付时间索引列表\n",
    "        self.div_rates = div_rates  # 各资产股息率\n",
    "        self.barriers = barriers    # 障碍水平（百分比）\n",
    "\n",
    "    def generate(self, n_paths):\n",
    "        paths = super().generate(n_paths)\n",
    "        # 检查股息支付时间步是否在路径范围内\n",
    "        for t in self.div_dates:\n",
    "            if t >= self.steps:\n",
    "                raise ValueError(f\"Dividend date {t} exceeds path steps {self.steps}\")\n",
    "            paths[:, t+1:] *= (1 - torch.tensor(self.div_rates, device=device))\n",
    "        return paths\n",
    "\n",
    "    def track_barriers(self, paths):\n",
    "        \"\"\"监测障碍事件（适配局部路径的时间窗口）\"\"\"\n",
    "        barrier_hit = torch.zeros_like(paths[:, :, 0], dtype=torch.bool)\n",
    "        for t_local in range(paths.size(1)):  # 局部时间步\n",
    "            current_min = torch.min(paths[:, t_local, :], dim=1).values\n",
    "            barrier_hit[:, t_local] = (current_min <= self.barriers * 100)\n",
    "        return barrier_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallableMBRCTrainer(DeepOptimalStoppingTrainer):\n",
    "    \"\"\"MBRC训练器 最小化发行人成本\"\"\"\n",
    "    def __init__(self, generator, coupon_rate, nominal=100):\n",
    "        super().__init__(generator)\n",
    "        self.coupon = coupon_rate * generator.T / generator.steps\n",
    "        self.nominal = nominal\n",
    "\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        barrier_indicator = self.generator.track_barriers(paths)\n",
    "        n_paths, total_time_steps, _ = paths.shape\n",
    "        \n",
    "        start_time = start_step * self.generator.dt\n",
    "        time_points = torch.linspace(\n",
    "            start_time,\n",
    "            self.generator.T,\n",
    "            total_time_steps,\n",
    "            device=device\n",
    "        )\n",
    "        discounts = torch.exp(-self.generator.r * time_points)\n",
    "        \n",
    "        payoffs = torch.zeros(n_paths, total_time_steps, device=device)\n",
    "        \n",
    "        for t in range(total_time_steps):\n",
    "            global_t = start_step + t\n",
    "            global_t = min(global_t, self.generator.steps)  # 确保不越界\n",
    "            \n",
    "            \n",
    "            coupon_part = self.coupon * (global_t + 1)\n",
    "            \n",
    "            if global_t == self.generator.steps:\n",
    "                final_prices = paths[:, t, :]\n",
    "                min_price = torch.min(final_prices, dim=1).values\n",
    "                # 使用 torch.minimum 替换 torch.min\n",
    "                payoff = torch.where(\n",
    "                    barrier_indicator[:, global_t],\n",
    "                    torch.minimum(min_price, torch.tensor(self.nominal, device=device)),\n",
    "                    self.nominal\n",
    "                )\n",
    "            else:\n",
    "                payoff = self.nominal\n",
    "            \n",
    "            payoffs[:, t] = (coupon_part + payoff) * discounts[t]\n",
    "        \n",
    "        return payoffs\n",
    "    \n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"生成考虑股息的继续路径\"\"\"\n",
    "        new_paths = torch.zeros(J, self.generator.steps-step, self.generator.d, device=device)\n",
    "        new_paths[:, 0] = current_state\n",
    "        \n",
    "        for t in range(1, self.generator.steps-step):\n",
    "            Z = torch.randn(J, self.generator.d, device=device)\n",
    "            increments = self.generator.drift + (self.generator.L @ Z.T).T * np.sqrt(self.generator.dt)\n",
    "            new_paths[:, t] = new_paths[:, t-1] * torch.exp(increments)\n",
    "            \n",
    "            # 应用股息支付\n",
    "            if (step + t) in self.generator.div_dates:\n",
    "                new_paths[:, t:] *= (1 - torch.tensor(self.generator.div_rates, device=device))\n",
    "        \n",
    "        return new_paths\n",
    "\n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        barrier_indicator = self.generator.track_barriers(new_paths)\n",
    "        batch_size, steps_remaining, _ = new_paths.shape\n",
    "        \n",
    "        start_time = current_step * self.generator.dt\n",
    "        time_points = torch.linspace(\n",
    "            start_time,\n",
    "            self.generator.T,\n",
    "            steps_remaining + 1,\n",
    "            device=device\n",
    "        )\n",
    "        discounts = torch.exp(-self.generator.r * time_points)[1:]\n",
    "        \n",
    "        payoffs = torch.zeros(batch_size, device=device)\n",
    "        for i in range(batch_size):\n",
    "            t = int(exercise_times[i].item())\n",
    "            relative_t = t - current_step - 1\n",
    "            \n",
    "            if relative_t < 0 or relative_t >= steps_remaining:\n",
    "                raise ValueError(f\"Invalid exercise time {t} at step {current_step}\")\n",
    "            \n",
    "            # 全局时间步截断\n",
    "            global_t = min(current_step + relative_t + 1, self.generator.steps)\n",
    "            \n",
    "            coupon_part = self.coupon * (global_t + 1)\n",
    "            \n",
    "            if global_t == self.generator.steps:\n",
    "                final_prices = new_paths[i, relative_t, :]\n",
    "                min_price = torch.min(final_prices)\n",
    "                payoff = torch.where(\n",
    "                    barrier_indicator[i, relative_t],  # 使用局部索引\n",
    "                    torch.minimum(min_price, torch.tensor(self.nominal, device=device)),\n",
    "                    self.nominal\n",
    "                )\n",
    "            else:\n",
    "                payoff = self.nominal\n",
    "            \n",
    "            discounted_payoff = (coupon_part + payoff) * discounts[relative_t]\n",
    "            payoffs[i] = discounted_payoff\n",
    "        \n",
    "        return payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBMGenerator(PathGenerator):\n",
    "    \"\"\"分数布朗运动生成器（带状态嵌入）\"\"\"\n",
    "    def __init__(self, H, steps, T):\n",
    "        super().__init__(steps+1, steps, T)  # 状态维度=时间步数\n",
    "        self.H = H\n",
    "        self._build_cov_matrix()\n",
    "    \n",
    "    def _build_cov_matrix(self):\n",
    "        t = np.linspace(0, self.T, self.steps+1)\n",
    "        cov = 0.5 * (\n",
    "            t[:, None]**(2 * self.H) \n",
    "            + t[None, :]**(2 * self.H) \n",
    "            - np.abs(t[:, None] - t[None, :])**(2 * self.H)\n",
    "        )\n",
    "        np.fill_diagonal(cov, cov.diagonal() + 1e-6)\n",
    "        \n",
    "        # 转换为PyTorch张量并保存\n",
    "        self.cov_matrix = torch.tensor(\n",
    "            cov.astype(np.float32),  # 先转换为float32的NumPy数组\n",
    "            device=device,\n",
    "            dtype=torch.float32       # 再转为PyTorch张量\n",
    "        )\n",
    "        \n",
    "        # Cholesky分解\n",
    "        self.L = torch.linalg.cholesky(self.cov_matrix)\n",
    "        \n",
    "    def generate(self, n_paths):\n",
    "        paths = torch.zeros(n_paths, self.steps+1, device=device, dtype=torch.float32)\n",
    "        for i in range(n_paths):\n",
    "            Z = torch.randn(self.steps+1, device=device, dtype=torch.float32)\n",
    "            paths[i] = self.L @ Z\n",
    "        \n",
    "        # 关键：生成三维嵌入路径，形状 (n_paths, steps+1, steps+1)\n",
    "        embedded = torch.zeros(n_paths, self.steps+1, self.steps+1, device=device)\n",
    "        for t in range(self.steps+1):\n",
    "            embedded[:, t, :t+1] = paths[:, :t+1]  # 填充完整历史状态\n",
    "        return embedded\n",
    "\n",
    "class FBMTrainer(DeepOptimalStoppingTrainer):\n",
    "    \"\"\"分数布朗运动训练器\"\"\"\n",
    "    def __init__(self, H, steps, T, hidden_dim=140):\n",
    "        generator = FBMGenerator(H, steps, T)\n",
    "        super().__init__(generator)\n",
    "        input_dim = (steps + 1) + 1  # 输入维度 = 状态(steps+1) + 收益(1)\n",
    "        self.policies = [\n",
    "            StoppingPolicy(input_dim, hidden_dim).to(device)\n",
    "            for _ in range(steps + 1)\n",
    "        ]\n",
    "        self.optimizers = [torch.optim.Adam(p.parameters(), lr=0.001) for p in self.policies]\n",
    "\n",
    "    def _prepare_training_data(self, paths, current_payoff, step):\n",
    "        # paths形状应为 (n_paths, steps+1, steps+1)\n",
    "        current_state = paths[:, step, :]  # 提取当前时间步的所有历史特征，形状 (n_paths, steps+1)\n",
    "        X = torch.cat([current_state, current_payoff.unsqueeze(1)], dim=1)  # 形状 (n_paths, steps+2)\n",
    "        continuation = self._nested_mc(paths, step)\n",
    "        y = (current_payoff >= continuation).float()\n",
    "        return X, y\n",
    "\n",
    "    def _apply_policies(self, paths, start_step):\n",
    "        exercise_times = torch.full((paths.size(0),), self.generator.steps, device=device)\n",
    "        for t in range(start_step + 1, self.generator.steps + 1):\n",
    "            # 检查路径是否包含足够的时间步\n",
    "            if t >= paths.size(1):\n",
    "                break  # 防止越界\n",
    "            \n",
    "            states = paths[:, t, :]\n",
    "            payoffs = self._compute_payoffs(paths[:, :t+1, :], start_step=start_step)[:, t - start_step]\n",
    "            inputs = torch.cat([states, payoffs.unsqueeze(1)], dim=1)\n",
    "            stop_probs = self.policies[t](inputs).squeeze()\n",
    "            stop_decisions = (stop_probs >= 0.5).float()\n",
    "            \n",
    "            mask = (exercise_times == self.generator.steps) & (stop_decisions == 1)\n",
    "            exercise_times[mask] = t\n",
    "        return exercise_times\n",
    "\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        \"\"\"从三维路径中提取FBM值\"\"\"\n",
    "        # paths形状应为 (n_paths, steps_remaining+1, features)\n",
    "        n_paths, steps_remaining_plus_1, features = paths.shape\n",
    "        payoffs = torch.zeros(n_paths, steps_remaining_plus_1, device=device)\n",
    "        for t in range(steps_remaining_plus_1):\n",
    "            payoffs[:, t] = paths[:, t, t]  # 提取对角线特征（假设特征在最后一维）\n",
    "        return payoffs\n",
    "\n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"生成条件路径（严格匹配步数）\"\"\"\n",
    "        t = step  # 当前时间步（从0开始）\n",
    "        total_steps = self.generator.steps\n",
    "        \n",
    "        # 需要生成的未来步数\n",
    "        future_steps = total_steps - t  # 从 t+1 到 total_steps，共 future_steps 步\n",
    "        \n",
    "        # 获取协方差矩阵\n",
    "        cov = self.generator.cov_matrix  # 形状 (total_steps+1, total_steps+1)\n",
    "        \n",
    "        # 分割协方差矩阵\n",
    "        Sigma11 = cov[:t+1, :t+1]          # 历史路径协方差 (t+1, t+1)\n",
    "        Sigma12 = cov[:t+1, t+1:t+1+future_steps]  # 历史与未来的协方差 (t+1, future_steps)\n",
    "        Sigma22 = cov[t+1:t+1+future_steps, t+1:t+1+future_steps]  # 未来路径协方差 (future_steps, future_steps)\n",
    "        \n",
    "        # 计算条件协方差\n",
    "        Sigma22_1 = Sigma22 - Sigma12.T @ torch.linalg.inv(Sigma11) @ Sigma12\n",
    "        L22 = torch.linalg.cholesky(Sigma22_1)\n",
    "        \n",
    "        # 生成随机噪声\n",
    "        Z = torch.randn(J, future_steps, device=device)\n",
    "        \n",
    "        # 计算均值项和随机项\n",
    "        current_state_truncated = current_state[:t+1]  # 截取到当前时间步（包含t）\n",
    "        mean_part = current_state_truncated @ torch.linalg.pinv(Sigma11) @ Sigma12  # 形状 (1, future_steps)\n",
    "        random_part = (L22 @ Z.T).T  # 形状 (J, future_steps)\n",
    "        \n",
    "        # 合并均值和随机部分\n",
    "        continuation_values = mean_part + random_part  # 形状 (J, future_steps)\n",
    "        \n",
    "        # 初始化三维路径张量 (J, total_steps+1, total_steps+1)\n",
    "        cond_paths = torch.zeros(J, total_steps+1, total_steps+1, device=device)\n",
    "        \n",
    "        # 填充历史路径（第一个特征维度）\n",
    "        cond_paths[:, :t+1, 0] = current_state_truncated.expand(J, -1)\n",
    "        \n",
    "        # 填充未来路径到对应特征位置（从 t+1 开始）\n",
    "        cond_paths[:, t+1:t+1+future_steps, t+1:t+1+future_steps] = continuation_values.unsqueeze(-1)\n",
    "        \n",
    "        return cond_paths\n",
    "    \n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        \"\"\"计算继续路径的收益 直接提取FBM值 \"\"\"\n",
    "        payoffs = torch.zeros(new_paths.size(0), device=device)\n",
    "        for i in range(new_paths.size(0)):\n",
    "            t = int(exercise_times[i].item())\n",
    "            # 转换为继续路径的局部索引\n",
    "            relative_t = t - current_step - 1\n",
    "            if relative_t < 0 or relative_t >= new_paths.size(1):\n",
    "                raise ValueError(f\"Invalid exercise time {t} at step {current_step}\")\n",
    "            payoffs[i] = new_paths[i, relative_t, relative_t]  # 提取局部路径的FBM值\n",
    "        return payoffs\n",
    "\n",
    "    def compute_upper_bound(self, n_paths=1024, J=16384, conf_level=0.95):\n",
    "        \"\"\"计算上界（修正继续路径步数）\"\"\"\n",
    "        # 生成主路径\n",
    "        primary_paths = self.generator.generate(n_paths)\n",
    "        payoffs = self._compute_payoffs(primary_paths)\n",
    "        \n",
    "        # 初始化Martingale过程\n",
    "        M = torch.zeros(n_paths, self.generator.steps+1, device=device)\n",
    "        \n",
    "        # 计算Martingale增量\n",
    "        for t in tqdm(range(self.generator.steps), desc=\"计算上界(Martingale)\"):\n",
    "            # 生成继续路径的步数应为 (steps - t)\n",
    "            steps_remaining = self.generator.steps - t\n",
    "            \n",
    "            # 初始化继续路径容器，形状 (n_paths, J, steps_remaining, features)\n",
    "            cont_paths = torch.zeros(\n",
    "                n_paths, J, steps_remaining, self.generator.d,  # 修正步数为 steps_remaining\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # 生成继续路径\n",
    "            for i in range(n_paths):\n",
    "                # 生成从 t 开始的继续路径（步数为 steps_remaining）\n",
    "                full_cont_path = self._generate_continuations(primary_paths[i, t], t, J)\n",
    "                # 截取从 t+1 到 steps 的路径（共 steps_remaining 步）\n",
    "                cont_paths[i] = full_cont_path[:, t+1:t+1+steps_remaining, :]\n",
    "            \n",
    "            # 计算继续价值\n",
    "            cont_values = torch.zeros(n_paths, device=device)\n",
    "            for i in range(n_paths):\n",
    "                ex_times = self._apply_policies(cont_paths[i], t)\n",
    "                cont_values[i] = self._compute_continuation_payoffs(cont_paths[i], ex_times, t).mean()\n",
    "            \n",
    "            # 更新Martingale\n",
    "            current_payoff = payoffs[:, t]\n",
    "            delta_M = (current_payoff >= cont_values).float() * (current_payoff - cont_values)\n",
    "            M[:, t+1] = M[:, t] + delta_M\n",
    "        \n",
    "        # 计算调整后的收益\n",
    "        adjusted = payoffs - M\n",
    "        max_values, _ = torch.max(adjusted, dim=1)\n",
    "        \n",
    "        # 统计量和置信区间计算（保持不变）\n",
    "        mean = max_values.mean().item()\n",
    "        std = max_values.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1 + conf_level)/2)\n",
    "        ci_half = z * std / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean,\n",
    "            'lower': mean - ci_half,\n",
    "            'upper': mean + ci_half,\n",
    "            'std': std\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBMTrainerWithCustomPrepare(DeepOptimalStoppingTrainer):\n",
    "    \"\"\"FBM专用训练器, 完全覆盖所有必要方法\"\"\"\n",
    "    def __init__(self, H, steps, T, hidden_dim=140):\n",
    "        generator = FBMGenerator(H, steps, T)\n",
    "        super().__init__(generator)\n",
    "        input_dim = (steps + 1) + 1  # 输入维度 = 状态(steps+1) + 收益(1)\n",
    "        self.policies = [\n",
    "            StoppingPolicy(input_dim, hidden_dim).to(device)\n",
    "            for _ in range(steps + 1)\n",
    "        ]\n",
    "        self.optimizers = [torch.optim.Adam(p.parameters(), lr=0.001) for p in self.policies]\n",
    "\n",
    "    # -------------------- 核心方法覆盖 --------------------\n",
    "    def _prepare_training_data(self, paths, current_payoff, step):\n",
    "        \"\"\"FBM专用数据准备（保持其他模型不变）\"\"\"\n",
    "        current_state = paths[:, step, :]  # 从三维嵌入路径提取状态\n",
    "        X = torch.cat([current_state, current_payoff.unsqueeze(1)], dim=1)\n",
    "        continuation = self._nested_mc(paths, step)\n",
    "        y = (current_payoff >= continuation).float()\n",
    "        return X, y\n",
    "\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        \"\"\"从三维路径中提取FBM值\"\"\"\n",
    "        n_paths, steps_remaining, _ = paths.shape\n",
    "        payoffs = torch.zeros(n_paths, steps_remaining, device=device)\n",
    "        for t in range(steps_remaining):\n",
    "            payoffs[:, t] = paths[:, t, t]  # 关键：提取对角线值\n",
    "        return payoffs\n",
    "\n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"生成条件路径（严格三维结构）\"\"\"\n",
    "        t = step + 1\n",
    "        cond_paths = torch.zeros(J, self.generator.steps+1, self.generator.steps+1, device=device)\n",
    "        cond_paths[:, :t, :t] = current_state.expand(J, -1, -1)  # 复制历史路径\n",
    "        \n",
    "        cov = self.generator.cov_matrix\n",
    "        Sigma11 = cov[:t, :t]\n",
    "        Sigma12 = cov[:t, t:]\n",
    "        Sigma22 = cov[t:, t:]\n",
    "        Sigma22_1 = Sigma22 - Sigma12.T @ torch.linalg.inv(Sigma11) @ Sigma12\n",
    "        L22 = torch.linalg.cholesky(Sigma22_1)\n",
    "        \n",
    "        Z = torch.randn(J, self.generator.steps+1 - t, device=device)\n",
    "        cond_paths[:, t:, t:] = (current_state[:t] @ torch.linalg.pinv(Sigma11) @ Sigma12) + (L22 @ Z.T).T\n",
    "        return cond_paths\n",
    "\n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        \"\"\"从继续路径中提取收益\"\"\"\n",
    "        payoffs = torch.zeros(new_paths.size(0), device=device)\n",
    "        for i in range(new_paths.size(0)):\n",
    "            t = int(exercise_times[i].item())\n",
    "            relative_t = t - current_step - 1\n",
    "            if 0 <= relative_t < new_paths.size(1):\n",
    "                payoffs[i] = new_paths[i, relative_t, relative_t]\n",
    "            else:\n",
    "                payoffs[i] = 0.0  # 无效时间步处理\n",
    "        return payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验执行代码\n",
    "def run_Bermudan_experiment():\n",
    "    \"\"\"Bermudan实验 对应论文表1/2 \"\"\"\n",
    "    params = {\n",
    "        \"d\": 100,\n",
    "        \"steps\": 9,\n",
    "        \"T\": 3.0,\n",
    "        \"r\": 0.05,\n",
    "        \"sigma\": 0.2,\n",
    "        \"rho\": 0.0,\n",
    "        \"div\": 0.1\n",
    "    }\n",
    "    \n",
    "    # 初始化生成器和训练器\n",
    "    bs_gen = BlackScholesGenerator(**params)\n",
    "    print(\"BlackScholesGenerator steps:\", bs_gen.steps)\n",
    "    trainer = BermudanMaxCallTrainer(bs_gen, hidden_dim=40+params[\"d\"])\n",
    "    \n",
    "    # 训练（3000步，每批8192条路径）\n",
    "    print(\"开始训练...\")\n",
    "    trainer.train(n_paths=8192, epochs=3000+params[\"d\"],batch_size=2048)#8192,3000\n",
    "    \n",
    "    # 计算上下界\n",
    "    print(\"计算下界...\")\n",
    "    lb_result = trainer.compute_lower_bound(n_paths=4096000)#4096000\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(\"计算上界...\")\n",
    "    ub_result = trainer.compute_upper_bound(n_paths=1024, J=16384) #1024，16384\n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"\\nBermudan实验结果:\")\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(f\"Upper bound: {ub_result['estimate']:.3f} ({ub_result['lower']:.3f}, {ub_result['upper']:.3f})\")\n",
    "    print(f\"95% Confidence Interval: [{lb_result['lower']:.3f}, {ub_result['upper']:.3f}]\")\n",
    "    print(f\"Point Estimate: {(lb_result['estimate'] + ub_result['estimate'])/2:.3f}\")\n",
    "\n",
    "\n",
    "def run_mbrc_experiment():\n",
    "    \"\"\"Callable MBRC实验 对应论文表3 \"\"\"\n",
    "    params = {\n",
    "        \"d\": 5,\n",
    "        \"steps\": 252,       # 252，1年，每日监测\n",
    "        \"T\": 1.0,\n",
    "        \"r\": 0.00,\n",
    "        \"sigma\": 0.2,\n",
    "        \"rho\": 0.0,\n",
    "        \"div_dates\": [126],  # 半年支付股息，126\n",
    "        \"div_rates\": [0.05]*5,\n",
    "        \"barriers\": 0.7      # 70%障碍\n",
    "    }\n",
    "    \n",
    "    mbrc_gen = MBRCGenerator(**params)\n",
    "    trainer = CallableMBRCTrainer(mbrc_gen, coupon_rate=0.07, nominal=100,hidden_dim=40+params[\"d\"])\n",
    "    \n",
    "    print(\"Training Callable MBRC policies...\")\n",
    "    trainer.train(n_paths=8192, epochs=3000+params[\"d\"])#8192,3000\n",
    "    \n",
    "    # 计算上下界\n",
    "    print(\"计算下界...\")\n",
    "    lb_result = trainer.compute_lower_bound(n_paths=4096000)#4096000\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(\"计算上界...\")\n",
    "    ub_result = trainer.compute_upper_bound(n_paths=1024, J=10000)#1024，16384\n",
    "    \n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"\\nMBRC实验结果:\")\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(f\"Upper bound: {ub_result['estimate']:.3f} ({ub_result['lower']:.3f}, {ub_result['upper']:.3f})\")\n",
    "    print(f\"95% Confidence Interval: [{lb_result['upper']:.3f}, {ub_result['lower']:.3f}]\")\n",
    "    print(f\"Point Estimate: {(lb_result['estimate'] + ub_result['estimate'])/2:.3f}\")\n",
    "\n",
    "\n",
    "def run_fbm_experiment(H=0.7):\n",
    "    \"\"\"分数布朗运动实验 对应论文表4 \"\"\"\n",
    "    assert 0 < H < 1, \"Hurst指数必须在 (0, 1) 之间\"\n",
    "    params = {\n",
    "        \"H\": H,\n",
    "        \"steps\": 100,#100\n",
    "        \"T\": 1.0\n",
    "    }\n",
    "    \n",
    "    fbm_gen = FBMGenerator(**params)\n",
    "    trainer = FBMTrainer(**params)\n",
    "    \n",
    "    print(f\"Training FBM (H={H}) policies...\")\n",
    "    trainer.train(n_paths=8192, epochs=3000)#6000\n",
    "    \n",
    "    print(\"计算下界...\")\n",
    "    lb_result = trainer.compute_lower_bound(n_paths=4096000)\n",
    "    print(\"计算上界...\")\n",
    "    ub_result = trainer.compute_upper_bound(n_paths=1024, J=16384 if H != 0.5 else 32768)#16384，32768\n",
    "    \n",
    "    print(f\"\\nFBM最优停止结果(H={H}):\")\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(f\"Upper bound: {ub_result['estimate']:.3f} ({ub_result['lower']:.3f}, {ub_result['upper']:.3f})\")\n",
    "    print(f\"95% Confidence Interval: [{lb_result['upper']:.3f}, {ub_result['lower']:.3f}]\")\n",
    "    print(f\"Point Estimate: {(lb_result['estimate'] + ub_result['estimate'])/2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Bermudan实验 =========\n",
      "BlackScholesGenerator steps: 9\n",
      "开始训练...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ce8857ea7e4cb2830a156a623ee0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "生成Black-Scholes路径:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cc0aaab406456b8f19d050e00a327a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training steps: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d74e52d7b6e4d83900574480e79a3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=8):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c445f6f4c54e59b663ed6ba4240428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 8:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0cd04e58f64734884bc16df06adf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=7):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bfebac0ef140179fd55df20e9483b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 7:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889746cdef2f4afd8d9fe28d584a80ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=6):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d7ca5d1ae44078ad6698393b15d71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 6:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9889856c7fab4da3a4c20609824a1ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=5):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532a62cf98b744aaa72faa627b14d5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 5:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d1d56605844c0d97be15c8e759dfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=4):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2d5c66de804d6e98f7297002e9c09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 4:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe90492bd7b5449eab6c6d77a996d029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=3):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5810688a2f48a4b9e0822bcdb5f6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 3:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209addbd38b34f9e8c4e7a733a119713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=2):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1743485b9054b40a164dbe2fbd2c6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 2:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a40ad7ff0741c0bf084cc26a7d0359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=1):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d572e85b66542b282c7037139dddc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 1:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53ab0bc4cd146bca9a99fd98c474591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "嵌套MC(t=0):   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0e2ffc4c124a01b18f383b75555ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training step 0:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算下界...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ea7b1b2cd947c5a29d85ff5fcd8ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "生成Black-Scholes路径:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound: 23.196 (22.707, 23.685)\n",
      "计算上界...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8087152de15544b6b3270f2c8a5f8ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "生成Black-Scholes路径:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9831265717bf4589a81a1eaa62792372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Martingale:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 1: 平均 immediate payoff = 0.0000, 平均 continuation value = 22.9819, 平均 ΔM = 0.0000\n",
      "时间步 2: 平均 immediate payoff = 11.8643, 平均 continuation value = 22.9426, 平均 ΔM = 0.4838\n",
      "时间步 3: 平均 immediate payoff = 16.0110, 平均 continuation value = 24.1237, 平均 ΔM = 1.5885\n",
      "时间步 4: 平均 immediate payoff = 17.5225, 平均 continuation value = 23.3203, 平均 ΔM = 1.9616\n",
      "时间步 5: 平均 immediate payoff = 19.5355, 平均 continuation value = 23.3985, 平均 ΔM = 2.9714\n",
      "时间步 6: 平均 immediate payoff = 20.8118, 平均 continuation value = 23.4392, 平均 ΔM = 3.3844\n",
      "时间步 7: 平均 immediate payoff = 21.6098, 平均 continuation value = 23.0156, 平均 ΔM = 3.4303\n",
      "时间步 8: 平均 immediate payoff = 22.1286, 平均 continuation value = 22.6083, 平均 ΔM = 3.8962\n",
      "时间步 9: 平均 immediate payoff = 22.6674, 平均 continuation value = 22.7730, 平均 ΔM = 4.0654\n",
      "\n",
      "Bermudan实验结果:\n",
      "Lower bound: 23.196 (22.707, 23.685)\n",
      "Upper bound: 29.766 (28.952, 30.581)\n",
      "95% Confidence Interval: [22.707, 30.581]\n",
      "Point Estimate: 26.481\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"========= Bermudan实验 =========\")\n",
    "    run_Bermudan_experiment()\n",
    "    \n",
    "    '''\n",
    "    print(\"========= Callable MBRC实验 =========\")\n",
    "    run_mbrc_experiment()\n",
    "    \n",
    "    \n",
    "    print(\"\\n========= 分数布朗运动实验 =========\")\n",
    "    for H in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "             0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n",
    "        run_fbm_experiment(H)''\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
