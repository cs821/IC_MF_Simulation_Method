{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_optimal_stopping.ipynb\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import cholesky\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import random\n",
    "from torch import nn\n",
    "# setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fnn\n",
    "class StoppingPolicy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer neural network with batch normalization and Xavier initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=40):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        # Xavier\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# can also try cnn algorithm\n",
    "# if want to use cnn, remember to change:\n",
    "# self.policies = [\n",
    "#    CNNStoppingPolicy(input_dim, hidden_dim).to(device)\n",
    "#    for _ in range(steps + 1)\n",
    "#]\n",
    "class CNNStoppingPolicy(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=40):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, 16, kernel_size=3, padding=1),  # [B, 1, d+1] → [B, 16, d+1]\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv1d(16, 32, kernel_size=3, padding=1),  # [B, 16, d+1] → [B, 32, d+1]\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),  # flattened to [B, 32 * (d+1)]\n",
    "            torch.nn.Linear(32 * input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv1d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim] → reshape to [B, 1, d+1]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathGenerator:\n",
    "    \"\"\"Abstract base class for path generation\"\"\"\n",
    "    def __init__(self, d, steps, T):\n",
    "        self.d = d          # dimension\n",
    "        self.steps = steps  # time steps\n",
    "        self.T = T         # total time\n",
    "\n",
    "    def generate(self, n_paths):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesGenerator(PathGenerator):\n",
    "    \"\"\"\n",
    "    Multidimensional Black-Scholes path generator\n",
    "    \"\"\"\n",
    "    def __init__(self, d, steps, T, r, sigma, rho, div):\n",
    "        super().__init__(d, steps, T)\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.div = div\n",
    "        self.dt = T / steps\n",
    "        self.drift = (self.r - self.div - 0.5 * self.sigma**2) * self.dt \n",
    "        self._build_cov_matrix()\n",
    "        \n",
    "    def _build_cov_matrix(self):\n",
    "        \"\"\"Construct the covariance matrix\"\"\"\n",
    "        cov = np.eye(self.d) * self.sigma**2\n",
    "        cov += np.ones((self.d, self.d)) * (self.rho * self.sigma**2)\n",
    "        np.fill_diagonal(cov, self.sigma**2)\n",
    "\n",
    "        #  Cholesky \n",
    "        self.L = torch.tensor(cholesky(cov, lower=True), device=device, dtype=torch.float32)\n",
    "        \n",
    "    def generate(self, n_paths):\n",
    "        \"\"\"generate paths \"\"\"\n",
    "        paths = torch.zeros(n_paths, self.steps+1, self.d, device=device, dtype=torch.float32)\n",
    "        paths[:, 0] = 100.0  # initial price\n",
    "        \n",
    "        noise = torch.randn(n_paths, self.steps, self.d, device=device)\n",
    "        \n",
    "        sqrt_dt = torch.tensor(np.sqrt(self.dt), device=device, dtype=torch.float32)\n",
    "        with tqdm(total=self.steps, desc=\"generate Black-Scholes path\", leave=False) as pbar:\n",
    "            for t in range(1, self.steps+1):\n",
    "                increments = self.drift + (self.L @ noise[:, t-1].T).T* sqrt_dt\n",
    "                paths[:, t] = paths[:, t-1] * torch.exp(increments)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepOptimalStoppingTrainer:\n",
    "    \"\"\"\n",
    "    Implement the recursive training algorithm from Section 2\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, hidden_dim=40):\n",
    "        self.generator = generator\n",
    "        self.policies = self._init_policies(hidden_dim)\n",
    "        self.optimizers = [torch.optim.Adam(p.parameters(), lr=0.001) \n",
    "                          for p in self.policies]\n",
    "        \n",
    "    def _init_policies(self, hidden_dim):\n",
    "        input_dim = self.generator.d + 1  # state+profit\n",
    "        return [StoppingPolicy(input_dim, hidden_dim).to(device)\n",
    "                for _ in range(self.generator.steps + 1)]\n",
    "    \n",
    "    def train(self, n_paths=8192, epochs=3000, batch_size=2048):\n",
    "        \"\"\"Run training using backward recursion\"\"\"\n",
    "        paths = self.generator.generate(n_paths)\n",
    "        for step in tqdm(reversed(range(self.generator.steps)), desc=\"Training steps\"):\n",
    "            current_payoff = self._compute_payoffs(paths, start_step=step)[:, step]\n",
    "            X, y = self._prepare_training_data(paths, current_payoff, step)\n",
    "            self._train_single_step(step, X, y, epochs, batch_size)\n",
    "    \n",
    "    def _compute_payoffs(self, paths):\n",
    "        \"\"\"Compute the immediate reward at each time step (to be implemented by subclass)\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _prepare_training_data(self, paths, current_payoff, step):\n",
    "        \"\"\"Prepare training data (including nested Monte Carlo simulation)\"\"\"\n",
    "        current_state = paths[:, step]\n",
    "        X = torch.cat([current_state, current_payoff.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # Estimate continuation value via nested Monte Carlo (pass starting time step)\n",
    "        continuation = self._nested_mc(paths, step)\n",
    "        y = (current_payoff >= continuation).float()\n",
    "        return X, y\n",
    "\n",
    "    def _nested_mc(self, paths, step, J=1000):\n",
    "        n_paths = paths.size(0)\n",
    "        continuation = torch.zeros(n_paths, device=device)\n",
    "        \n",
    "        with tqdm(total=n_paths, desc=f\"nested MC(t={step})\", leave=False) as main_pbar:\n",
    "            for i in range(n_paths):\n",
    "                # Generate J continuation paths from current step\n",
    "                new_paths = self._generate_continuations(paths[i, step], step, J)\n",
    "                \n",
    "                # Apply continuation strategy (starting time step is passed)\n",
    "                exercise_times = self._apply_policies(new_paths, start_step=step)\n",
    "                \n",
    "                # Compute payoffs of continuation paths starting from step\n",
    "                payoffs = self._compute_continuation_payoffs(new_paths, exercise_times, step)\n",
    "                \n",
    "                continuation[i] = payoffs.mean()\n",
    "                main_pbar.update(1)\n",
    "        \n",
    "        return continuation\n",
    "    \n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"Generate continuation paths from current state (to be implemented by subclass)\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _apply_policies(self, paths, start_step):\n",
    "        \"\"\"Apply trained policy network (Eq. (5)) \"\"\"\n",
    "        exercise_times = torch.full((paths.size(0),), self.generator.steps, device=device)\n",
    "        for t in range(start_step + 1, paths.size(1)):\n",
    "            states = paths[:, t]\n",
    "            # Dynamically compute payoff at current time step (start_step as reference)\n",
    "            payoffs = self._compute_payoffs(\n",
    "                paths[:, :t+1], \n",
    "                start_step=start_step\n",
    "            )[:, t - start_step]  \n",
    "            \n",
    "            inputs = torch.cat([states, payoffs.unsqueeze(1)], dim=1)\n",
    "            stop_probs = self.policies[t](inputs).squeeze()\n",
    "            stop_decisions = (stop_probs >= 0.5).float()\n",
    "            \n",
    "            mask = (exercise_times == self.generator.steps) & (stop_decisions == 1)\n",
    "            exercise_times[mask] = t\n",
    "        \n",
    "        return exercise_times\n",
    "\n",
    "    def _train_single_step(self, step, X, y, epochs, batch_size):\n",
    "        \"\"\"Train policy network at a single time step\"\"\"\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=f\"Training step {step}\", leave=False):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizers[step].zero_grad()\n",
    "                pred = self.policies[step](batch_X).squeeze()\n",
    "                loss = torch.nn.BCELoss()(pred, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizers[step].step()\n",
    "    \n",
    "    def compute_lower_bound(self, n_paths=4096000, conf_level=0.95):\n",
    "        \"\"\"Compute lower bound and confidence interval (Section 3.1) \"\"\"\n",
    "        # Generate independent set of paths\n",
    "        test_paths = self.generator.generate(n_paths)\n",
    "        \n",
    "        # Determine stopping time using trained policy\n",
    "        exercise_times = self._apply_policies(test_paths, 0)\n",
    "        \n",
    "        # Compute payoffs\n",
    "        payoffs = torch.zeros(n_paths, device=device)\n",
    "        for i in range(n_paths):\n",
    "            t = int(exercise_times[i].item())\n",
    "            payoffs[i] = self._compute_payoffs(test_paths[i:i+1])[0, t]\n",
    "        \n",
    "        mean = payoffs.mean().item()\n",
    "        std = payoffs.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1 + conf_level)/2)\n",
    "        ci_half = z * std / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean,\n",
    "            'lower': mean - ci_half,\n",
    "            'upper': mean + ci_half,\n",
    "            'std': std\n",
    "        }\n",
    "    \n",
    "    def compute_upper_bound(self, n_paths=1024, J=16384, conf_level=0.95):\n",
    "        \"\"\"Compute upper bound and confidence interval\"\"\"\n",
    "        # Generate independent primary paths\n",
    "        primary_paths = self.generator.generate(n_paths)\n",
    "        payoffs = self._compute_payoffs(primary_paths, start_step=0)  # shape: [n_paths, steps+1]\n",
    "        steps = self.generator.steps  \n",
    "        \n",
    "        # ============== New: Compute exercise times for primary paths ==============\n",
    "        # Initialize exercise times, default to maturity (steps)\n",
    "        exercise_times = torch.full((n_paths,), steps, device=device)\n",
    "        \n",
    "        # Traverse each timestep, use policy networks to determine exercise time\n",
    "        for t in range(steps):\n",
    "            # Get state and payoff at time t for primary paths\n",
    "            states = primary_paths[:, t]  # shape: [n_paths, d]\n",
    "            current_payoff = payoffs[:, t]  # shape: [n_paths]\n",
    "            \n",
    "            # Concatenate inputs (state + current payoff)\n",
    "            inputs = torch.cat([states, current_payoff.unsqueeze(1)], dim=1)\n",
    "            \n",
    "            # Use policy network at time t to decide stop\n",
    "            stop_probs = self.policies[t](inputs).squeeze()\n",
    "            stop_decisions = (stop_probs >= 0.5).float()  # 0 or 1\n",
    "            \n",
    "            # Update exercise times: only affect paths not yet stopped\n",
    "            mask = (exercise_times == steps) & (stop_decisions == 1)\n",
    "            exercise_times[mask] = t\n",
    "        \n",
    "        # Compute average exercise time\n",
    "        avg_exercise_time = exercise_times.float().mean().item()\n",
    "        print(f\"Average exercise time: {avg_exercise_time:.2f} (steps), corresponds to: {avg_exercise_time * self.generator.dt:.2f} years\")\n",
    "        # =============================================================================\n",
    "        \n",
    "        # Following original code (compute martingale and upper bound)\n",
    "        M = torch.zeros(n_paths, steps + 1, device=device)\n",
    "        E_H_prev = payoffs[:, 0].clone()  # Initial expected value\n",
    "        \n",
    "        for t in tqdm(range(1, steps+1), desc=\"Building Martingale\"):\n",
    "            steps_remaining = steps - (t - 1)  \n",
    "            time_points = steps_remaining + 1  \n",
    "            cont_paths = torch.zeros(n_paths, J, time_points, self.generator.d, device=device)\n",
    "            cont_values = torch.zeros(n_paths, J, device=device)\n",
    "            \n",
    "            for i in range(n_paths):\n",
    "                current_state = primary_paths[i, t-1]\n",
    "                cont_paths[i] = self._generate_continuations(current_state, t-1, J)\n",
    "                ex_times = self._apply_policies(cont_paths[i], start_step=t-1)\n",
    "                cont_values[i] = self._compute_continuation_payoffs(\n",
    "                    cont_paths[i], ex_times, current_step=t-1\n",
    "                )\n",
    "            \n",
    "            E_H_t = cont_values.mean(dim=1)\n",
    "            delta_M = torch.clamp(payoffs[:, t-1] - E_H_prev, min=0)\n",
    "            M[:, t] = M[:, t-1] + delta_M\n",
    "            E_H_prev = E_H_t.clone()\n",
    "            \n",
    "            # Debug info (optional)\n",
    "            avg_cont = E_H_prev.mean().item()\n",
    "            avg_payoff = payoffs[:, t-1].mean().item()\n",
    "            print(f\"Step {t}: Avg immediate payoff = {avg_payoff:.4f}, Avg continuation value = {avg_cont:.4f}, Mean ΔM = {delta_M.mean().item():.4f}\")\n",
    "        \n",
    "        # Compute adjusted payoffs\n",
    "        adjusted_payoffs = torch.max(payoffs - M[:, :payoffs.size(1)], dim=1)[0]\n",
    "        \n",
    "        # Statistical results\n",
    "        mean_est = adjusted_payoffs.mean().item()\n",
    "        std_est = adjusted_payoffs.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1+conf_level)/2)\n",
    "        ci_half = z * std_est / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean_est,\n",
    "            'lower': mean_est - ci_half,\n",
    "            'upper': mean_est + ci_half,\n",
    "            'std': std_est,\n",
    "            'avg_exercise_time': avg_exercise_time  # New: return average exercise time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BermudanMaxCallTrainer(DeepOptimalStoppingTrainer):\n",
    "    \"\"\"Implement Bermudan Max-Call experiment (Section 4.1)\"\"\"\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        # paths shape: (n_paths, L, d), where L is the actual number of time steps\n",
    "        max_values, _ = torch.max(paths, dim=2)  # (n_paths, L)\n",
    "        L = paths.size(1) \n",
    "        # Construct time vector: start_time, start_time + dt, ..., start_time + (L - 1) * dt\n",
    "        time_points = start_step * self.generator.dt + torch.arange(L, device=device, dtype=torch.float32) * self.generator.dt\n",
    "        discounts = torch.exp(-self.generator.r * time_points)\n",
    "        return (max_values - 100).clamp(min=0) * discounts  # K=100\n",
    "\n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"Generate continuation paths from step to steps (inclusive)\"\"\"\n",
    "        steps_remaining = self.generator.steps - step \n",
    "        # Need to generate steps_remaining + 1 time points (from step to steps, inclusive)\n",
    "        new_paths = torch.zeros(J, steps_remaining + 1, self.generator.d, device=device)\n",
    "        new_paths[:, 0] = current_state\n",
    "        sqrt_dt = torch.sqrt(torch.tensor(self.generator.dt, \n",
    "                                    device=device, dtype=torch.float32))\n",
    "        for t in range(1, steps_remaining + 1): \n",
    "            Z = torch.randn(J, self.generator.d, device=device)\n",
    "            increments = self.generator.drift + (self.generator.L @ Z.T).T * sqrt_dt\n",
    "            new_paths[:, t] = new_paths[:, t-1] * torch.exp(increments)\n",
    "        return new_paths\n",
    "    \n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        # new_paths shape: (batch_size, L, d)， L = steps_remaining + 1\n",
    "        batch_size, L, _ = new_paths.shape\n",
    "        time_points = current_step * self.generator.dt + torch.arange(L, device=device, dtype=torch.float32) * self.generator.dt\n",
    "        discounts = torch.exp(-self.generator.r * time_points)[1:]\n",
    "        \n",
    "        max_values, _ = torch.max(new_paths, dim=2)  # (batch_size, L)\n",
    "        payoffs = (max_values[:, 1:] - 100).clamp(min=0) * discounts \n",
    "        \n",
    "        selected_payoffs = torch.zeros(batch_size, device=device)\n",
    "        for i in range(batch_size):\n",
    "            absolute_t = int(exercise_times[i].item())\n",
    "            if absolute_t == self.generator.steps: \n",
    "                selected_payoffs[i] = payoffs[i, -1]\n",
    "            else:\n",
    "                relative_t = absolute_t - (current_step + 1)\n",
    "                if relative_t < 0 or relative_t >= (L - 1):\n",
    "                    raise ValueError(f\"Invalid exercise time {absolute_t} at step {current_step}\")\n",
    "                selected_payoffs[i] = payoffs[i, relative_t]\n",
    "        \n",
    "        return selected_payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBRCGenerator(BlackScholesGenerator):\n",
    "    \"\"\"Path generator with dividends and barrier monitoring\"\"\"\n",
    "    def __init__(self, d, steps, T, r, sigma, rho, div_dates, div_rates, barriers):\n",
    "        super().__init__(d, steps, T, r, sigma, rho, div=0)\n",
    "        self.div_dates = div_dates  # List of dividend payment time indices\n",
    "        self.div_rates = div_rates  # Dividend rates for each asset\n",
    "        self.barriers = barriers    # Barrier level (percentage)\n",
    "\n",
    "    def generate(self, n_paths):\n",
    "        paths = super().generate(n_paths)\n",
    "        for t in self.div_dates:\n",
    "            if t >= self.steps:\n",
    "                raise ValueError(f\"Dividend date {t} exceeds path steps {self.steps}\")\n",
    "            paths[:, t+1:] *= (1 - torch.tensor(self.div_rates, device=device))\n",
    "        return paths\n",
    "\n",
    "    def track_barriers(self, paths):\n",
    "        \"\"\"Monitor barrier events within the local time window\"\"\"\n",
    "        barrier_hit = torch.zeros_like(paths[:, :, 0], dtype=torch.bool)\n",
    "        for t_local in range(paths.size(1)): \n",
    "            current_min = torch.min(paths[:, t_local, :], dim=1).values\n",
    "            barrier_hit[:, t_local] = (current_min <= self.barriers * 100)\n",
    "        return barrier_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallableMBRCTrainer(DeepOptimalStoppingTrainer):\n",
    "    \"\"\"MBRC trainer minimizing issuer's cost\"\"\"\n",
    "    def __init__(self, generator, coupon_rate, nominal=100):\n",
    "        super().__init__(generator)\n",
    "        self.coupon = coupon_rate * generator.T / generator.steps\n",
    "        self.nominal = nominal\n",
    "\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        barrier_indicator = self.generator.track_barriers(paths)\n",
    "        n_paths, total_time_steps, _ = paths.shape\n",
    "        \n",
    "        start_time = start_step * self.generator.dt\n",
    "        time_points = torch.linspace(\n",
    "            start_time,\n",
    "            self.generator.T,\n",
    "            total_time_steps,\n",
    "            device=device\n",
    "        )\n",
    "        discounts = torch.exp(-self.generator.r * time_points)\n",
    "        \n",
    "        payoffs = torch.zeros(n_paths, total_time_steps, device=device)\n",
    "        \n",
    "        for t in range(total_time_steps):\n",
    "            global_t = start_step + t\n",
    "            global_t = min(global_t, self.generator.steps)  \n",
    "            \n",
    "            \n",
    "            coupon_part = self.coupon * (global_t + 1)\n",
    "            \n",
    "            if global_t == self.generator.steps:\n",
    "                final_prices = paths[:, t, :]\n",
    "                min_price = torch.min(final_prices, dim=1).values\n",
    "                # use torch.minimum to replace torch.min\n",
    "                payoff = torch.where(\n",
    "                    barrier_indicator[:, global_t],\n",
    "                    torch.minimum(min_price, torch.tensor(self.nominal, device=device)),\n",
    "                    self.nominal\n",
    "                )\n",
    "            else:\n",
    "                payoff = self.nominal\n",
    "            \n",
    "            payoffs[:, t] = (coupon_part + payoff) * discounts[t]\n",
    "        \n",
    "        return payoffs\n",
    "    \n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"Generate continuation paths with dividends\"\"\"\n",
    "        new_paths = torch.zeros(J, self.generator.steps-step, self.generator.d, device=device)\n",
    "        new_paths[:, 0] = current_state\n",
    "        \n",
    "        for t in range(1, self.generator.steps-step):\n",
    "            Z = torch.randn(J, self.generator.d, device=device)\n",
    "            increments = self.generator.drift + (self.generator.L @ Z.T).T * np.sqrt(self.generator.dt)\n",
    "            new_paths[:, t] = new_paths[:, t-1] * torch.exp(increments)\n",
    "            \n",
    "            # Apply dividend payments\n",
    "            if (step + t) in self.generator.div_dates:\n",
    "                new_paths[:, t:] *= (1 - torch.tensor(self.generator.div_rates, device=device))\n",
    "        \n",
    "        return new_paths\n",
    "\n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        barrier_indicator = self.generator.track_barriers(new_paths)\n",
    "        batch_size, steps_remaining, _ = new_paths.shape\n",
    "        \n",
    "        start_time = current_step * self.generator.dt\n",
    "        time_points = torch.linspace(\n",
    "            start_time,\n",
    "            self.generator.T,\n",
    "            steps_remaining + 1,\n",
    "            device=device\n",
    "        )\n",
    "        discounts = torch.exp(-self.generator.r * time_points)[1:]\n",
    "        \n",
    "        payoffs = torch.zeros(batch_size, device=device)\n",
    "        for i in range(batch_size):\n",
    "            t = int(exercise_times[i].item())\n",
    "            relative_t = t - current_step - 1\n",
    "            \n",
    "            if relative_t < 0 or relative_t >= steps_remaining:\n",
    "                raise ValueError(f\"Invalid exercise time {t} at step {current_step}\")\n",
    "            \n",
    "            global_t = min(current_step + relative_t + 1, self.generator.steps)\n",
    "            \n",
    "            coupon_part = self.coupon * (global_t + 1)\n",
    "            \n",
    "            if global_t == self.generator.steps:\n",
    "                final_prices = new_paths[i, relative_t, :]\n",
    "                min_price = torch.min(final_prices)\n",
    "                payoff = torch.where(\n",
    "                    barrier_indicator[i, relative_t],\n",
    "                    torch.minimum(min_price, torch.tensor(self.nominal, device=device)),\n",
    "                    self.nominal\n",
    "                )\n",
    "            else:\n",
    "                payoff = self.nominal\n",
    "            \n",
    "            discounted_payoff = (coupon_part + payoff) * discounts[relative_t]\n",
    "            payoffs[i] = discounted_payoff\n",
    "        \n",
    "        return payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBMGenerator(PathGenerator):\n",
    "    \"\"\"Fractional Brownian motion (fBM) generator with state embedding\"\"\"\n",
    "    def __init__(self, H, steps, T):\n",
    "        super().__init__(steps+1, steps, T) \n",
    "        self.H = H\n",
    "        self._build_cov_matrix()\n",
    "    \n",
    "    def _build_cov_matrix(self):\n",
    "        t = np.linspace(0, self.T, self.steps+1)\n",
    "        cov = 0.5 * (\n",
    "            t[:, None]**(2 * self.H) \n",
    "            + t[None, :]**(2 * self.H) \n",
    "            - np.abs(t[:, None] - t[None, :])**(2 * self.H)\n",
    "        )\n",
    "        np.fill_diagonal(cov, cov.diagonal() + 1e-6)\n",
    "        \n",
    "        self.cov_matrix = torch.tensor(\n",
    "            cov.astype(np.float32), \n",
    "            device=device,\n",
    "            dtype=torch.float32      \n",
    "        )\n",
    "        \n",
    "        # Cholesky decomposition\n",
    "        self.L = torch.linalg.cholesky(self.cov_matrix)\n",
    "        \n",
    "    def generate(self, n_paths):\n",
    "        paths = torch.zeros(n_paths, self.steps+1, device=device, dtype=torch.float32)\n",
    "        for i in range(n_paths):\n",
    "            Z = torch.randn(self.steps+1, device=device, dtype=torch.float32)\n",
    "            paths[i] = self.L @ Z\n",
    "        \n",
    "        # Key: generate 3D embedded paths with shape (n_paths, steps + 1, steps + 1)\n",
    "        embedded = torch.zeros(n_paths, self.steps+1, self.steps+1, device=device)\n",
    "        for t in range(self.steps+1):\n",
    "            embedded[:, t, :t+1] = paths[:, :t+1] \n",
    "        return embedded\n",
    "\n",
    "class FBMTrainer(DeepOptimalStoppingTrainer):\n",
    "    def __init__(self, H, steps, T, hidden_dim=140):\n",
    "        generator = FBMGenerator(H, steps, T)\n",
    "        super().__init__(generator)\n",
    "        input_dim = (steps + 1) + 1  # state(steps+1) + profit(1)\n",
    "        self.policies = [\n",
    "            StoppingPolicy(input_dim, hidden_dim).to(device)\n",
    "            for _ in range(steps + 1)\n",
    "        ]\n",
    "        self.optimizers = [torch.optim.Adam(p.parameters(), lr=0.001) for p in self.policies]\n",
    "\n",
    "    def _prepare_training_data(self, paths, current_payoff, step):\n",
    "        current_state = paths[:, step, :] \n",
    "        X = torch.cat([current_state, current_payoff.unsqueeze(1)], dim=1)  #  (n_paths, steps+2)\n",
    "        continuation = self._nested_mc(paths, step)\n",
    "        y = (current_payoff >= continuation).float()\n",
    "        return X, y\n",
    "\n",
    "    def _apply_policies(self, paths, start_step):\n",
    "        exercise_times = torch.full((paths.size(0),), self.generator.steps, device=device)\n",
    "        for t in range(start_step + 1, self.generator.steps + 1):\n",
    "            if t >= paths.size(1):\n",
    "                break  \n",
    "            \n",
    "            states = paths[:, t, :]\n",
    "            payoffs = self._compute_payoffs(paths[:, :t+1, :], start_step=start_step)[:, t - start_step]\n",
    "            inputs = torch.cat([states, payoffs.unsqueeze(1)], dim=1)\n",
    "            stop_probs = self.policies[t](inputs).squeeze()\n",
    "            stop_decisions = (stop_probs >= 0.5).float()\n",
    "            \n",
    "            mask = (exercise_times == self.generator.steps) & (stop_decisions == 1)\n",
    "            exercise_times[mask] = t\n",
    "        return exercise_times\n",
    "\n",
    "    def _compute_payoffs(self, paths, start_step=0):\n",
    "        \"\"\"Extract FBM values from 3D paths\"\"\"\n",
    "        n_paths, steps_remaining_plus_1, features = paths.shape\n",
    "        payoffs = torch.zeros(n_paths, steps_remaining_plus_1, device=device)\n",
    "        for t in range(steps_remaining_plus_1):\n",
    "            payoffs[:, t] = paths[:, t, t]  \n",
    "        return payoffs\n",
    "\n",
    "    def _generate_continuations(self, current_state, step, J):\n",
    "        \"\"\"Generate conditional paths with strictly matched number of steps\"\"\"\n",
    "        t = step\n",
    "        total_steps = self.generator.steps\n",
    "     \n",
    "        future_steps = total_steps - t  \n",
    "        \n",
    "        cov = self.generator.cov_matrix  #  (total_steps+1, total_steps+1)\n",
    "        \n",
    "        # Split the covariance matrix\n",
    "        Sigma11 = cov[:t+1, :t+1]          # Covariance of historical paths (t+1, t+1)\n",
    "        Sigma12 = cov[:t+1, t+1:t+1+future_steps]  # History-future covariance (t+1, future_steps)\n",
    "        Sigma22 = cov[t+1:t+1+future_steps, t+1:t+1+future_steps]  # Future path covariance (future_steps, future_steps)\n",
    "        \n",
    "        Sigma22_1 = Sigma22 - Sigma12.T @ torch.linalg.inv(Sigma11) @ Sigma12\n",
    "        L22 = torch.linalg.cholesky(Sigma22_1)\n",
    "        \n",
    "        # random noise\n",
    "        Z = torch.randn(J, future_steps, device=device)\n",
    "        \n",
    "        current_state_truncated = current_state[:t+1]\n",
    "        mean_part = current_state_truncated @ torch.linalg.pinv(Sigma11) @ Sigma12  #  (1, future_steps)\n",
    "        random_part = (L22 @ Z.T).T  #  (J, future_steps)\n",
    "        \n",
    "        continuation_values = mean_part + random_part  # (J, future_steps)\n",
    "        \n",
    "        cond_paths = torch.zeros(J, total_steps+1, total_steps+1, device=device)\n",
    "        cond_paths[:, :t+1, 0] = current_state_truncated.expand(J, -1)\n",
    "        cond_paths[:, t+1:t+1+future_steps, t+1:t+1+future_steps] = continuation_values.unsqueeze(-1)\n",
    "        \n",
    "        return cond_paths\n",
    "    \n",
    "    def _compute_continuation_payoffs(self, new_paths, exercise_times, current_step):\n",
    "        \"\"\"Compute continuation payoffs by directly extracting FBM values \"\"\"\n",
    "        payoffs = torch.zeros(new_paths.size(0), device=device)\n",
    "        for i in range(new_paths.size(0)):\n",
    "            t = int(exercise_times[i].item())\n",
    "            relative_t = t - current_step - 1\n",
    "            if relative_t < 0 or relative_t >= new_paths.size(1):\n",
    "                raise ValueError(f\"Invalid exercise time {t} at step {current_step}\")\n",
    "            payoffs[i] = new_paths[i, relative_t, relative_t]  \n",
    "        return payoffs\n",
    "\n",
    "    def compute_upper_bound(self, n_paths=1024, J=16384, conf_level=0.95):\n",
    "        #  Generate main paths\n",
    "        primary_paths = self.generator.generate(n_paths)\n",
    "        payoffs = self._compute_payoffs(primary_paths)\n",
    "        \n",
    "        # Initialize the martingale process\n",
    "        M = torch.zeros(n_paths, self.generator.steps+1, device=device)\n",
    "        \n",
    "        # Compute martingale increments\n",
    "        for t in tqdm(range(self.generator.steps), desc=\"Computing upper bound (Martingale)\"):\n",
    "            steps_remaining = self.generator.steps - t\n",
    "            cont_paths = torch.zeros(\n",
    "                n_paths, J, steps_remaining, self.generator.d, \n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # generate continuation paths\n",
    "            for i in range(n_paths):\n",
    "                full_cont_path = self._generate_continuations(primary_paths[i, t], t, J)\n",
    "                cont_paths[i] = full_cont_path[:, t+1:t+1+steps_remaining, :]\n",
    "            \n",
    "            # Compute the continuation value\n",
    "            cont_values = torch.zeros(n_paths, device=device)\n",
    "            for i in range(n_paths):\n",
    "                ex_times = self._apply_policies(cont_paths[i], t)\n",
    "                cont_values[i] = self._compute_continuation_payoffs(cont_paths[i], ex_times, t).mean()\n",
    "            \n",
    "            # update Martingale\n",
    "            current_payoff = payoffs[:, t]\n",
    "            delta_M = (current_payoff >= cont_values).float() * (current_payoff - cont_values)\n",
    "            M[:, t+1] = M[:, t] + delta_M\n",
    "        \n",
    "        # compute adjusted payoffs\n",
    "        adjusted = payoffs - M\n",
    "        max_values, _ = torch.max(adjusted, dim=1)\n",
    "        \n",
    "        # Compute statistics and confidence intervals\n",
    "        mean = max_values.mean().item()\n",
    "        std = max_values.std(unbiased=True).item()\n",
    "        z = 1.96 if conf_level == 0.95 else norm.ppf((1 + conf_level)/2)\n",
    "        ci_half = z * std / np.sqrt(n_paths)\n",
    "        \n",
    "        return {\n",
    "            'estimate': mean,\n",
    "            'lower': mean - ci_half,\n",
    "            'upper': mean + ci_half,\n",
    "            'std': std\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Execution\n",
    "def run_Bermudan_experiment():\n",
    "    params = {\n",
    "        \"d\": 3,\n",
    "        \"steps\": 9,\n",
    "        \"T\": 3.0,\n",
    "        \"r\": 0.05,\n",
    "        \"sigma\": 0.2,\n",
    "        \"rho\": 0.0,\n",
    "        \"div\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Initialize the generator and trainer\n",
    "    bs_gen = BlackScholesGenerator(**params)\n",
    "    print(\"BlackScholesGenerator steps:\", bs_gen.steps)\n",
    "    trainer = BermudanMaxCallTrainer(bs_gen, hidden_dim=40+params[\"d\"])\n",
    "    \n",
    "    # Training (1500 steps, 4000 paths per batch)\n",
    "    print(\"Start training...\")\n",
    "    trainer.train(n_paths=40, epochs=15+params[\"d\"],batch_size=2048)#8192,3000\n",
    "    \n",
    "    # Compute lower and upper bound\n",
    "    print(\"Computing lower bound...\")\n",
    "    lb_result = trainer.compute_lower_bound(n_paths=2000)#4096000\n",
    "    print(\"Computing upper bound...\")\n",
    "    ub_result = trainer.compute_upper_bound(n_paths=1024, J=1300) #1024，16384\n",
    "    \n",
    "    # print results\n",
    "    print(f\"\\nBermudan results:\")\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(f\"Upper bound: {ub_result['estimate']:.3f} ({ub_result['lower']:.3f}, {ub_result['upper']:.3f})\")\n",
    "    print(f\"95% Confidence Interval: [{lb_result['lower']:.3f}, {ub_result['upper']:.3f}]\")\n",
    "    print(f\"Point Estimate: {(lb_result['estimate'] + ub_result['estimate'])/2:.3f}\")\n",
    "\n",
    "\n",
    "def run_mbrc_experiment():\n",
    "    params = {\n",
    "        \"d\": 2,\n",
    "        \"steps\": 10,       # 2 weeks\n",
    "        \"T\": 1.0,\n",
    "        \"r\": 0.00,\n",
    "        \"sigma\": 0.2,\n",
    "        \"rho\": 0.0,\n",
    "        \"div_dates\": [5],  \n",
    "        \"div_rates\": [0.05]*2,\n",
    "        \"barriers\": 0.7      \n",
    "    }\n",
    "    \n",
    "    mbrc_gen = MBRCGenerator(**params)\n",
    "    trainer = CallableMBRCTrainer(mbrc_gen, coupon_rate=0.07, nominal=100,hidden_dim=40+params[\"d\"])\n",
    "    \n",
    "    print(\"Training Callable MBRC policies...\")\n",
    "    trainer.train(n_paths=1000, epochs=800+params[\"d\"])#8192,3000\n",
    "    \n",
    "    # Compute lower and upper bound\n",
    "    print(\"Computing lower bound...\")\n",
    "    lb_result = trainer.compute_lower_bound(n_paths=2000000)#4096000\n",
    "    print(\"Computing upper bound...\")\n",
    "    ub_result = trainer.compute_upper_bound(n_paths=1024, J=1000)#1024，16384\n",
    "    \n",
    "    \n",
    "    # print results\n",
    "    print(f\"\\nMBRC results:\")\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(f\"Upper bound: {ub_result['estimate']:.3f} ({ub_result['lower']:.3f}, {ub_result['upper']:.3f})\")\n",
    "    print(f\"95% Confidence Interval: [{lb_result['upper']:.3f}, {ub_result['lower']:.3f}]\")\n",
    "    print(f\"Point Estimate: {(lb_result['estimate'] + ub_result['estimate'])/2:.3f}\")\n",
    "\n",
    "\n",
    "def run_fbm_experiment(H=0.7):\n",
    "    assert 0 < H < 1, \"Hurst must be between (0, 1) \"\n",
    "    params = {\n",
    "        \"H\": H,\n",
    "        \"steps\": 20,#100\n",
    "        \"T\": 1.0\n",
    "    }\n",
    "    \n",
    "    fbm_gen = FBMGenerator(**params)\n",
    "    trainer = FBMTrainer(**params)\n",
    "    \n",
    "    print(f\"Training FBM (H={H}) policies...\")\n",
    "    trainer.train(n_paths=4000, epochs=1500)#6000\n",
    "    \n",
    "    print(\"Computing lower bound...\")\n",
    "    lb_result = trainer.compute_lower_bound(n_paths=2000000)\n",
    "    print(\"Computing upper bound...\")\n",
    "    ub_result = trainer.compute_upper_bound(n_paths=1024, J=13000 if H != 0.5 else 26000)#16384，32768\n",
    "    \n",
    "    print(f\"\\nFBM optimal stopping results(H={H}):\")\n",
    "    print(f\"Lower bound: {lb_result['estimate']:.3f} ({lb_result['lower']:.3f}, {lb_result['upper']:.3f})\")\n",
    "    print(f\"Upper bound: {ub_result['estimate']:.3f} ({ub_result['lower']:.3f}, {ub_result['upper']:.3f})\")\n",
    "    print(f\"95% Confidence Interval: [{lb_result['upper']:.3f}, {ub_result['lower']:.3f}]\")\n",
    "    print(f\"Point Estimate: {(lb_result['estimate'] + ub_result['estimate'])/2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"========= Bermudan Experiment =========\")\n",
    "    run_Bermudan_experiment()\n",
    "    \n",
    "    '''\n",
    "    print(\"========= Callable MBRC Experiment =========\")\n",
    "    run_mbrc_experiment()\n",
    "    \n",
    "    \n",
    "    print(\"\\n========= FBM Experiment =========\")\n",
    "    for H in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "             0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n",
    "        run_fbm_experiment(H)''\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
